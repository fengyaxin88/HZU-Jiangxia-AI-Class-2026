# Transformer 原理交互式可视化：自注意力部分

这是一个为你定制的**高细节提示词（Prompt）**。你可以直接将其提供给 GPT-4、Claude 3 或专门的 AI 编程助手（如 Cursor / GitHub Copilot Chat）来生成代码框架。

---

### **提示词：Transformer 原理交互式可视化网站**

**角色**：你是一位精通前端开发（React/Vue, D3.js, Three.js）和深度学习的可视化专家。你的任务是创建一个**单页面交互式演示网站**，用于直观地展示 Transformer 模型在处理文本时的内部矩阵运算过程。

**核心目标**：

不要只做“动效”，要做“教学工具”。让一个完全不懂 Transformer 的人，通过观看动画和交互，能够理解**“为什么要用矩阵”**以及**“矩阵是如何通过变换提取语义的”**。

---

### **一、 整体布局与视觉风格**

1. **界面分区**：

   - **左侧（输入区）**：文本输入框，用户可以输入一个简单的英文句子（如 "The cat sat on the mat"）。
   - **中间（可视化主舞台）**：最大的区域，用于展示矩阵动画。
   - **右侧（控制面板与解说）**：

     - 步骤选择器（Embedding -> Self-Attention -> FFN）。
     - 播放/暂停/重置/单步前进按钮。
     - 实时文本解说框：解释当前动画在做什么。
2. **画风要求**：

   - **配色**：使用 **Monokai** 或 **Solarized** 配色系（程序员熟悉的配色）。不同的矩阵用固定的色块表示（例如：Query=红色, Key=绿色, Value=蓝色, Output=紫色）。
   - **风格**：**极简科技风**。背景为深灰或黑色。矩阵用网格表示，数值用颜色深浅或高度（3D）表示。
   - **字体**：无衬线字体（Inter 或 JetBrains Mono）。

---

### **二、 核心可视化需求（重点详细）**

请按以下流程逐步可视化。**请务必展示矩阵的形状变化和数值流动**。

#### **1. 输入层：从 Token 到向量**

- **动画 1：Tokenization**

  - 将句子拆分为单词（或 Subword）。
  - 每个单词下方显示其在词表中的 ID（整数）。
- **动画 2：Embedding Lookup**

  - *视觉效果*：从一个巨大的“Embedding 矩阵”（只显示局部）中，根据 ID 抽取出对应的行向量。
  - *矩阵展示*：展示一个 `[vocab_size, d_model]` 的大矩阵（虚化背景）和抽取出的 `[seq_len, d_model]` 矩阵（高亮前景）。
- **动画 3：添加位置编码（Positional Encoding）**

  - 显示一个与 Embedding 同维度的正弦/余弦波矩阵。
  - *关键动作*：两个矩阵（Embedding + PE）对应元素**相加**，颜色融合。

#### **2. 核心机制：自注意力（Self-Attention）—— 这是重中之重**

不要直接展示最终公式，要拆解成步骤。假设 `d_model=512`，`num_heads=8`，`d_k=64`。

- **步骤 A：线性投影 (Q, K, V 的生成)**

  - 输入：`X` (`[seq_len, 512]`)。
  - 视觉：在 X 右侧放置三个可学习的权重矩阵 `Wq`, `Wk`, `Wv`（形状均为 `[512, 64]`，这里先演示单头）。
  - 动画：矩阵乘法。展示 `X` 与 `Wq` 相乘得到 `Q`，同理得到 `K`, `V`。
  - *解说*：“这一步是将输入向量‘旋转’到不同的空间，分别用来表示‘我在找什么’、‘我包含什么信息’、‘我有什么内容’。”
- **步骤 B：计算注意力分数 (Attention Scores)**

  - 视觉：展示 `Q` 和 `K^T` (K的转置)。
  - 动画：**点积运算**。用热力图展示 `Q * K^T` 的结果矩阵 `[seq_len, seq_len]`。
  - *交互*：鼠标悬停在热力图的 `(i,j)` 格子上，高亮显示第 `i` 个词的 Query 向量和第 `j` 个词的 Key 向量，并用线条连接它们的点积计算过程。
- **步骤 C：缩放与 Mask (Scaling & Masking)**

  - 缩放：展示数值除以 `sqrt(d_k)`，热力图颜色整体变暗（方差归一化）。
  - Mask（可选，做 Encoder 时可跳过，或专门做一个 Decoder 演示）：下三角矩阵遮盖，右上角变为负无穷（颜色变为黑色）。
- **步骤 D：Softmax**

  - 动画：按行做 Softmax。展示每一行的数值如何通过指数函数压缩到 0-1 之间，且每行和为 1。
  - *视觉冲击*：热力图中，只有少数几个格子变得极亮（权重高），其他变暗。
- **步骤 E：加权求和 (Weighted Sum)**

  - 视觉：将 Softmax 后的注意力权重矩阵与 `V` 相乘。
  - 动画：展示对于第一个词的输出，是如何由所有词的 Value 向量按照权重“混合”而成的。
- **步骤 F：多头注意力 (Multi-Head)**

  - *切换视角*：将上述的单头过程复制 8 份（横向排列），每一份的颜色略有不同（红橙黄绿青蓝紫...）。
  - *Concat 操作*：展示 8 个 `[seq_len, 64]` 的矩阵在最后一个维度拼接，变成 `[seq_len, 512]`。
  - *最终投影*：乘以 `Wo` 矩阵得到输出。

#### **3. 残差连接与层归一化 (Add & Norm)**

- **Add**：用两条不同颜色的河流（输入流 + 注意力输出流）汇聚到一起，表示残差相加。
- **Norm**：展示矩阵的每一行向量如何被“拉伸”或“压缩”，使其均值为 0，方差为 1。

#### **4. 前馈神经网络 (FFN)**

- 展示两个线性层矩阵的乘法。
- 重点：中间的 **ReLU** 激活函数。可视化矩阵中所有负数被“截断”为 0 的瞬间（数值变蓝或消失）。

---

### **三、 交互功能要求**

1. **上帝视角（宏观）**：

   - 显示 Transformer 的整体架构图（Encoder/Decoder 堆叠），点击某一层进入该层的微观视角。
2. **显微镜视角（微观）**：

   - **可拖拽的矩阵**：用户可以点击并拖动矩阵中的数值滑块，观察改变一个数值如何影响后续结果。
   - **维度标签**：矩阵的长宽必须实时标注（如 "Seq Len = 5", "d_model = 512"）。
3. **进度控制**：

   - 提供一个时间轴（Timeline），用户可以拖拽进度条跳转到任意步骤。

---

### **四、 技术栈建议（供参考）**

- **框架**：React 或 Svelte (轻量级)。
- **可视化库**：

  - **2D**：D3.js (用于矩阵热力图和数据流)。
  - **动画库**：Framer Motion 或 GSAP (用于流畅的过渡)。

---

### **输出要求**

请生成一个完整的、可运行的代码项目结构。用于网页展示
