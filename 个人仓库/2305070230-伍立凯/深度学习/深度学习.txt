在pytorch中函数跟一个下划线表示重写内容，比如x.grad.zero_()
新张量=老张量.clone()			通过分配新内存，将老张量的一个副本分配给新张量，使得在更改新张量的值时不会影响老张量
torch.tensor(元素值)			创建张量
torch.arange(元素数量)			0开始顺序创建张量
张量.cumsum(axis=[求和的维度]，0就是左数第一维)	按维度累积求和，返回的张量维度与原来相同
torch.dot(张量1,张量2)			将两个张量按元素相乘，再求和。等价torch.sum(x*y)
torch.mv(张量1,张量2)			按照x*y矩阵乘1*y矩阵来算
torch.mm(张量1,张量2)			按照两个矩阵相乘来算
torch.norm(张量)				将张量(矩阵)中每一个元素平方后求和再开根求L2范数
torch.abs(张量).sum()			将张量中每一个元素求绝对值后再求和求L1范数，abs()为求绝对值
张量.T						对张量进行转置
张量.shape 					张量的形状
张量.numel()					张量的元素总数
新=老张量.reshape(形状)			改变张量形状，不改变元素数量和值
torch.cat((拼接元素)，dim=0(按行堆叠)/1(按列堆叠))		张量拼接
张量1==张量2					通过判断对位元素值是否相等来生成以布尔值为元素值的同形状张量
张量.sum((axis=[求和的维度]，0就是左数第一维),(keepdims=true/false,决定axis运算是否将被运算的维度去除，为true则不去除,可与广播机制联动))			对张量内元素求和以产生一个只有和为元素值的张量
张量.mean((axis=[求均值的维度]，0就是左数第一维),(keepdims=true/false,决定axis运算是否将被运算的维度去除，为true则不去除,可与广播机制联动))			对张量内元素求均值以产生一个只有均值为元素值的张量
	广播机制：当对形状不相同的张量进行基础运算时，张量会通过复制扩张成两个一样形状的张量再运算
	为多元素赋值：索引多元素，再赋值（可以一次为多个元素赋值）
	原地操作：通过操作尽可能使新结果保持在老内存，如x+=y，x[:]=x+y等
张量.numpy() , int(张量)等		格式转换
.backward()			对前面给定的值求偏导
.detach()				返回一个和原张量共享数据，但完全脱离计算图、不再追踪任何梯度的新张量
