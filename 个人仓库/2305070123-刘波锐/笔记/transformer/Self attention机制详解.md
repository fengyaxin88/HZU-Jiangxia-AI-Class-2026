# 3 Self attention**机制详解**



## 1 Self-attention的机制和原理

self-attention是一种通过自身和自身进行关联的attention机制, 从而得到更好的representation来表达自身.

self-attention是attention机制的一种特殊情况，在self-attention中, Q=K=V, 序列中的每个单词(token)都和该序列中的其他所有单词(token)进行attention规则的计算.

attention机制计算的特点在于, 可以直接跨越一句话中不同距离的token, 可以远距离的学习到序列的知识依赖和语序结构.

![image-20260221125001736](C:\Users\刘波锐\AppData\Roaming\Typora\typora-user-images\image-20260221125001736.png)

- 从上图中可以看到, self-attention可以远距离的捕捉到语义层面的特征(its的指代对象是Law).
- 应用传统的RNN, LSTM, 在获取长距离语义特征和结构特征的时候, 需要按照序列顺序依次计算, 距离越远的联系信息的损耗越大, 有效提取和捕获的可能性越小.
- 但是应用self-attention时, 计算过程中会直接将句子中任意两个token的联系通过一个计算步骤直接联系起来,



关于self-attention为什么要使用(Q, K, V)三元组而不是其他形式:

- 首先一条就是从分析的角度看, 查询Query是一条独立的序列信息, 通过关键词Key的提示作用, 得到最终语义的真实值Value表达, 数学意义更充分, 完备.
- 这里不使用(K, V)或者(V)没有什么必须的理由, 也没有相关的论文来严格阐述比较试验的结果差异, 所以可以作为开放性问题未来去探索, 只要明确在经典self-attention实现中用的是三元组就好.





## 2 Self-attention中的归一化概述

- 训练上的意义: 随着词嵌入维度d_k的增大, q * k 点积后的结果也会增大, 在训练时会将带有饱和区间的激活函数（比如：sigmoid激活函数、tanh激活函数、逻辑回归softmax）推入梯度非常小的区域, 可能出现梯度消失的现象, 造成模型收敛困难.
- 数学上的意义: 假设q和k的统计变量是满足标准正态分布的独立随机变量, 意味着q和k满足均值为0, 方差为1. 那么q和k的点积结果就是均值为0, 方差为d_k, 为了抵消这种方差被放大d_k倍的影响, 在计算中主动将点积缩放1/sqrt(d_k), 这样点积后的结果依然满足均值为0, 方差为1.



## 3 维度与点积大小的关系

- 针对为什么维度会影响点积的大小, 原始论文中有这样的一点解释如下:

```
To illustrate why the dot products get large, assume that the components of q and k 
are independent random variables with mean 0 and variance 1. Then their doct product,
q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k.
```

**前提设定**：首先明确向量 q 和 k 的每个对应分量（比如 q₁和 k₁、q₂和 k₂）都是 “独立随机变量”—— 意思是每个分量的取值不会互相影响，且满足 “均值 0、方差 1” 的常见统计分布（比如标准正态分布），这是机器学习 / 深度学习中对向量初始化的典型假设（比如 Transformer 的查询 q、键 k 向量）。

**点积的计算本质**：点积是两个同维度向量对应位置分量相乘后求和，比如三维向量的点积就是 q₁k₁+q₂k₂+q₃k₃，dₖ维向量就有 dₖ个这样的乘积项相加。

**均值为 0 的原因**：因为每个分量 qᵢ和 kᵢ都是均值 0 的独立变量，根据期望的性质，E [qᵢkᵢ] = E [qᵢ]×E [kᵢ] = 0×0 = 0；而求和的期望等于期望的和，所以整个点积的均值（期望）就是 dₖ个 0 相加，结果仍为 0。

**方差为 dₖ的核心逻辑**：方差衡量的是变量取值的波动程度，根据方差的性质，独立变量之和的方差等于各变量方差之和。首先计算单个乘积项 qᵢkᵢ的方差：由于 qᵢ和 kᵢ独立且方差均为 1，Var (qᵢkᵢ) = E [(qᵢkᵢ)²] - (E [qᵢkᵢ])² = E [qᵢ²]×E [kᵢ²] - 0；而对于均值 0、方差 1 的变量，E [x²] = Var (x) + (E [x])² = 1，因此单个乘积项的方差为 1×1=1。dₖ个这样的独立项相加，总方差就是 1×dₖ = dₖ。

**实际意义**：这意味着向量维度 dₖ越大，点积的波动范围就越大（方差随维度线性增长），点积的绝对值也会大概率变得更大 —— 这也是 Transformer 中需要引入 “缩放点积注意力（Scaled Dot-Product Attention）” 的核心原因：通过除以√dₖ把点积的方差还原为 1，避免高维下点积过大导致 softmax 函数饱和、梯度消失。