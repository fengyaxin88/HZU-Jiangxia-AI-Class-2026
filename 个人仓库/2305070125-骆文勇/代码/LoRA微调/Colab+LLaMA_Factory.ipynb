{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOyZBTlPmfZoR4LEbsFL2+F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 安装依赖以及检查GPU"],"metadata":{"id":"5Q1fpbjm2kB5"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyUJtgC90UR9","executionInfo":{"status":"ok","timestamp":1771989907475,"user_tz":-480,"elapsed":245,"user":{"displayName":"sluo luo","userId":"06539739621327480246"}},"outputId":"7eaca095-de45-42a6-d745-82a539f942b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Feb 25 03:25:09 2026       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n","+-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","\n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9F86uyy11DWr","executionInfo":{"status":"ok","timestamp":1771989942934,"user_tz":-480,"elapsed":20764,"user":{"displayName":"sluo luo","userId":"06539739621327480246"}},"outputId":"f21de30f-844d-4302-9476-384ec8029f02"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# 3. 安装LLaMA Factory\n","!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n","%cd LLaMA-Factory\n","!pip install -e .[torch,metrics]\n","!pip install bitsandbytes>=0.43.0  # 4bit量化依赖"],"metadata":{"id":"jtuy28sf1HAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 通过命令行进行LoRA微调"],"metadata":{"id":"PCgz58Mn2rfm"}},{"cell_type":"code","source":["%cd /content/LLaMA-Factory\n","\n","# 修正后的命令：无缩进、无行尾注释、\\紧贴行尾\n","!llamafactory-cli train \\\n","    --stage sft \\\n","    --do_train \\\n","    --model_name_or_path Qwen/Qwen2-7B-Instruct \\\n","    --dataset alpaca_gpt4_en \\\n","    --template qwen \\\n","    --finetuning_type lora \\\n","    --quantization_bit 4 \\\n","    --lora_rank 8 \\\n","    --lora_alpha 16 \\\n","    --lora_dropout 0.05 \\\n","    --lora_target all \\\n","    --output_dir /content/drive/MyDrive/LLaMA-Factory/qwen2-7b-lora \\\n","    --overwrite_output_dir \\\n","    --cutoff_len 1024 \\\n","    --preprocessing_num_workers 16 \\\n","    --per_device_train_batch_size 4 \\\n","    --per_device_eval_batch_size 4 \\\n","    --gradient_accumulation_steps 4 \\\n","    --learning_rate 5e-5 \\\n","    --num_train_epochs 3.0 \\\n","    --lr_scheduler_type cosine \\\n","    --warmup_ratio 0.03 \\\n","    --logging_steps 10 \\\n","    --save_steps 100 \\\n","    --plot_loss \\\n","    --fp16"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xChsEWAn1SMW","outputId":"d2b9aa95-8904-4424-e1aa-92f843ed61bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/LLaMA-Factory\n","/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n","  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n","/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n","  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n","/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n","  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n","warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n","[WARNING|2026-02-25 03:27:53] llamafactory.hparams.parser:149 >> We recommend enable `upcast_layernorm` in quantized training.\n","[INFO|2026-02-25 03:27:54] llamafactory.hparams.parser:459 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n","config.json: 100% 663/663 [00:00<00:00, 3.11MB/s]\n","[INFO|configuration_utils.py:667] 2026-02-25 03:27:54,551 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n","[INFO|configuration_utils.py:739] 2026-02-25 03:27:54,556 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3584,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 18944,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 28,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 28,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 4,\n","  \"pad_token_id\": null,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_parameters\": {\n","    \"rope_theta\": 1000000.0,\n","    \"rope_type\": \"default\"\n","  },\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"5.0.0\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 152064\n","}\n","\n","tokenizer_config.json: 1.29kB [00:00, 3.26MB/s]\n","vocab.json: 2.78MB [00:00, 66.9MB/s]\n","merges.txt: 1.67MB [00:00, 78.6MB/s]\n","tokenizer.json: 7.03MB [00:00, 109MB/s]\n","Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","[INFO|configuration_utils.py:667] 2026-02-25 03:27:57,603 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n","[INFO|configuration_utils.py:739] 2026-02-25 03:27:57,604 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3584,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 18944,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 28,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 28,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 4,\n","  \"pad_token_id\": null,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_parameters\": {\n","    \"rope_theta\": 1000000.0,\n","    \"rope_type\": \"default\"\n","  },\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"5.0.0\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 152064\n","}\n","\n","[INFO|configuration_utils.py:667] 2026-02-25 03:27:57,712 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n","[INFO|configuration_utils.py:739] 2026-02-25 03:27:57,713 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3584,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 18944,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 28,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 28,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 4,\n","  \"pad_token_id\": null,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_parameters\": {\n","    \"rope_theta\": 1000000.0,\n","    \"rope_type\": \"default\"\n","  },\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"5.0.0\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 152064\n","}\n","\n","[INFO|2026-02-25 03:27:58] llamafactory.data.loader:144 >> Loading dataset llamafactory/alpaca_gpt4_en...\n","README.md: 100% 491/491 [00:00<00:00, 3.15MB/s]\n","alpaca_gpt4_data_en.json: 100% 43.3M/43.3M [00:02<00:00, 15.3MB/s]\n","Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 51983 examples [00:00, 57332.44 examples/s]\n","Converting format of dataset (num_proc=16): 100% 51983/51983 [00:07<00:00, 7296.26 examples/s]\n","Running tokenizer on dataset (num_proc=16): 100% 51983/51983 [01:20<00:00, 648.24 examples/s] \n","training example:\n","input_ids:\n","[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 35127, 2326, 10414, 369, 19429, 9314, 13, 151645, 198, 151644, 77091, 198, 16, 13, 44514, 264, 23831, 323, 77116, 9968, 25, 7405, 2704, 697, 20969, 525, 28308, 315, 264, 8045, 315, 25322, 323, 23880, 11, 15651, 12833, 11, 4361, 40836, 11, 323, 9314, 49027, 13, 1096, 8609, 311, 3410, 697, 2487, 448, 279, 7565, 36393, 311, 729, 518, 1181, 1850, 323, 646, 1492, 5358, 20601, 18808, 382, 17, 13, 3285, 424, 304, 5792, 6961, 5702, 25, 32818, 374, 16587, 369, 20337, 3746, 24854, 11, 23648, 11, 323, 40613, 2820, 13, 70615, 369, 518, 3245, 220, 16, 20, 15, 4420, 315, 23193, 90390, 10158, 476, 220, 22, 20, 4420, 315, 70820, 10158, 1817, 2003, 382, 18, 13, 2126, 3322, 6084, 25, 24515, 3322, 4271, 6084, 374, 16587, 369, 6961, 323, 10502, 1632, 32751, 13, 1084, 8609, 311, 36277, 19671, 11, 7269, 24675, 729, 11, 323, 11554, 9314, 6513, 323, 22077, 729, 13, 70615, 369, 220, 22, 12, 24, 4115, 315, 6084, 1817, 3729, 13, 151645, 198]\n","inputs:\n","<|im_start|>system\n","You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n","<|im_start|>user\n","Give three tips for staying healthy.<|im_end|>\n","<|im_start|>assistant\n","1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n","\n","2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n","\n","3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n","\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 44514, 264, 23831, 323, 77116, 9968, 25, 7405, 2704, 697, 20969, 525, 28308, 315, 264, 8045, 315, 25322, 323, 23880, 11, 15651, 12833, 11, 4361, 40836, 11, 323, 9314, 49027, 13, 1096, 8609, 311, 3410, 697, 2487, 448, 279, 7565, 36393, 311, 729, 518, 1181, 1850, 323, 646, 1492, 5358, 20601, 18808, 382, 17, 13, 3285, 424, 304, 5792, 6961, 5702, 25, 32818, 374, 16587, 369, 20337, 3746, 24854, 11, 23648, 11, 323, 40613, 2820, 13, 70615, 369, 518, 3245, 220, 16, 20, 15, 4420, 315, 23193, 90390, 10158, 476, 220, 22, 20, 4420, 315, 70820, 10158, 1817, 2003, 382, 18, 13, 2126, 3322, 6084, 25, 24515, 3322, 4271, 6084, 374, 16587, 369, 6961, 323, 10502, 1632, 32751, 13, 1084, 8609, 311, 36277, 19671, 11, 7269, 24675, 729, 11, 323, 11554, 9314, 6513, 323, 22077, 729, 13, 70615, 369, 220, 22, 12, 24, 4115, 315, 6084, 1817, 3729, 13, 151645, 198]\n","labels:\n","1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n","\n","2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n","\n","3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n","\n","[INFO|configuration_utils.py:667] 2026-02-25 03:29:33,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/config.json\n","[INFO|configuration_utils.py:739] 2026-02-25 03:29:33,834 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"dtype\": \"bfloat16\",\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3584,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 18944,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 28,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 28,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 4,\n","  \"pad_token_id\": null,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_parameters\": {\n","    \"rope_theta\": 1000000.0,\n","    \"rope_type\": \"default\"\n","  },\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"transformers_version\": \"5.0.0\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 152064\n","}\n","\n","[INFO|2026-02-25 03:29:33] llamafactory.model.model_utils.quantization:144 >> Quantizing model to 4 bit with bitsandbytes.\n","[INFO|2026-02-25 03:29:33] llamafactory.model.model_utils.kv_cache:144 >> KV cache is disabled during training.\n","model.safetensors.index.json: 27.8kB [00:00, 59.7MB/s]\n","[INFO|modeling_utils.py:732] 2026-02-25 03:29:40,329 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/model.safetensors.index.json\n","Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n","Downloading (incomplete total...):  97% 14.8G/15.2G [02:32<00:02, 161MB/s]\n","Downloading (incomplete total...):  97% 14.8G/15.2G [02:35<00:05, 67.2MB/s]\n","Downloading (incomplete total...): 100% 15.2G/15.2G [02:37<00:00, 177MB/s]\n","Fetching 4 files: 100% 4/4 [02:37<00:00, 39.40s/it]\n","Download complete: 100% 15.2G/15.2G [02:37<00:00, 177MB/s]                [INFO|modeling_utils.py:801] 2026-02-25 03:32:18,034 >> Will use dtype=torch.bfloat16 as defined in model's config object\n","[INFO|configuration_utils.py:1014] 2026-02-25 03:32:18,043 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"use_cache\": false\n","}\n","\n","Download complete: 100% 15.2G/15.2G [02:37<00:00, 96.6MB/s]\n","Loading weights: 100% 339/339 [01:01<00:00,  5.50it/s, Materializing param=model.norm.weight] \n","generation_config.json: 100% 243/243 [00:00<00:00, 929kB/s]\n","[INFO|configuration_utils.py:967] 2026-02-25 03:33:20,581 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct/snapshots/f2826a00ceef68f0f2b946d945ecc0477ce4450c/generation_config.json\n","[INFO|configuration_utils.py:1014] 2026-02-25 03:33:20,582 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    151645,\n","    151643\n","  ],\n","  \"pad_token_id\": 151643,\n","  \"repetition_penalty\": 1.05,\n","  \"temperature\": 0.7,\n","  \"top_k\": 20,\n","  \"top_p\": 0.8\n","}\n","\n","[INFO|2026-02-25 03:33:20] llamafactory.model.model_utils.checkpointing:144 >> Gradient checkpointing enabled.\n","[INFO|2026-02-25 03:33:20] llamafactory.model.model_utils.attention:144 >> Using torch SDPA for faster training and inference.\n","[INFO|2026-02-25 03:33:20] llamafactory.model.adapter:144 >> Upcasting trainable params to float32.\n","[INFO|2026-02-25 03:33:20] llamafactory.model.adapter:144 >> Fine-tuning method: LoRA\n","[INFO|2026-02-25 03:33:20] llamafactory.model.model_utils.misc:144 >> Found linear modules: gate_proj,down_proj,k_proj,o_proj,v_proj,q_proj,up_proj\n","[INFO|2026-02-25 03:33:21] llamafactory.model.loader:144 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n","[WARNING|trainer.py:922] 2026-02-25 03:33:21,394 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","[INFO|trainer.py:2383] 2026-02-25 03:33:21,864 >> ***** Running training *****\n","[INFO|trainer.py:2384] 2026-02-25 03:33:21,864 >>   Num examples = 51,983\n","[INFO|trainer.py:2385] 2026-02-25 03:33:21,864 >>   Num Epochs = 3\n","[INFO|trainer.py:2386] 2026-02-25 03:33:21,864 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:2389] 2026-02-25 03:33:21,864 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:2390] 2026-02-25 03:33:21,864 >>   Gradient Accumulation steps = 4\n","[INFO|trainer.py:2391] 2026-02-25 03:33:21,864 >>   Total optimization steps = 9,747\n","[INFO|trainer.py:2392] 2026-02-25 03:33:21,868 >>   Number of trainable parameters = 20,185,088\n","{'loss': '1.197', 'grad_norm': '0.4054', 'learning_rate': '1.536e-06', 'epoch': '0.003078'}\n","{'loss': '1.195', 'grad_norm': '0.4442', 'learning_rate': '3.242e-06', 'epoch': '0.006156'}\n","{'loss': '1.093', 'grad_norm': '0.446', 'learning_rate': '4.949e-06', 'epoch': '0.009234'}\n","{'loss': '1.145', 'grad_norm': '0.4409', 'learning_rate': '6.655e-06', 'epoch': '0.01231'}\n","{'loss': '1.075', 'grad_norm': '0.4806', 'learning_rate': '8.362e-06', 'epoch': '0.01539'}\n","  1% 55/9747 [19:42<58:48:25, 21.84s/it]"]}]},{"cell_type":"markdown","source":["# 主观问题微调前后对比"],"metadata":{"id":"pbxvm-Js26I2"}},{"cell_type":"code","source":["from llmtuner import ChatModel\n","\n","# 1. 定义测试问题\n","test_prompts = [\n","    \"Explain quantum computing in simple terms.\",\n","    \"Write a Python function to calculate the factorial of a number.\",\n","    \"What are the benefits of meditation?\",\n","    \"Translate the following sentence into French: 'Hello, how are you today?'\",\n","    \"Give me 3 ideas for a weekend trip in the mountains.\"\n","]\n","\n","# 2. 加载【微调前】的基座模型\n","print(\"=\"*30 + \" 微调前（基座模型）回答 \" + \"=\"*30)\n","base_model = ChatModel(dict(\n","    model_name_or_path=\"Qwen/Qwen2-7B-Instruct\",\n","    template=\"qwen\",\n","    quantization_bit=4  # 同样用4bit量化\n","))\n","for prompt in test_prompts:\n","    print(f\"\\n【问题】{prompt}\")\n","    print(f\"【回答】{base_model.chat(prompt)[0]}\")\n","\n","# 3. 加载【微调后】的模型（基座+LoRA权重）\n","print(\"\\n\" + \"=\"*30 + \" 微调后（LoRA）回答 \" + \"=\"*30)\n","lora_model = ChatModel(dict(\n","    model_name_or_path=\"Qwen/Qwen2-7B-Instruct\",\n","    adapter_name_or_path=\"/content/drive/MyDrive/LLaMA-Factory/qwen2-7b-lora\",\n","    template=\"qwen\",\n","    quantization_bit=4\n","))\n","for prompt in test_prompts:\n","    print(f\"\\n【问题】{prompt}\")\n","    print(f\"【回答】{lora_model.chat(prompt)[0]}\")"],"metadata":{"id":"9LyFLpJy3CqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 客观指标计算（RLEU/ROUGE）"],"metadata":{"id":"0zaivr6J3VEA"}},{"cell_type":"code","source":["!pip install evaluate rouge-score nltk sacrebleu\n","import evaluate\n","import nltk\n","nltk.download('punkt')\n","\n","# 1. 加载指标\n","rouge = evaluate.load('rouge')\n","bleu = evaluate.load('sacrebleu')\n","\n","# 2. 先让微调后的模型生成所有回答（为了简化，这里假设我们有参考答案，实际可用GPT-4的回答作为参考）\n","# 注意：Alpaca数据集本身有'reference'列，我们可以取测试集的一部分来算指标\n","# 这里为了演示，我们手动构造一个简单的\"预测-参考\"对示例\n","predictions = [\n","    \"Quantum computing uses qubits that can be 0, 1, or both at once, allowing it to solve certain problems much faster than classical computers.\",\n","    \"Here's a Python factorial function: def fact(n): return 1 if n == 0 else n * fact(n-1)\"\n","]\n","references = [\n","    [\"Quantum computing leverages quantum mechanics phenomena like superposition and entanglement to process information in ways classical computers can't.\"],\n","    [\"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"]\n","]\n","\n","# 3. 计算ROUGE\n","rouge_results = rouge.compute(predictions=predictions, references=references)\n","print(\"ROUGE指标:\", rouge_results)\n","\n","# 4. 计算BLEU\n","bleu_results = bleu.compute(predictions=predictions, references=references)\n","print(\"BLEU指标:\", bleu_results)"],"metadata":{"id":"S9fRuhMO3fph"},"execution_count":null,"outputs":[]}]}